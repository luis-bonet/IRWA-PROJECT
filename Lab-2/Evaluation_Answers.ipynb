{"cells":[{"cell_type":"markdown","metadata":{"id":"nht2NZkqroGX","pycharm":{"name":"#%% md\n"}},"source":["### Prepare Data\n","We are going to test the above metrics on a ranking of results which is stored in the ```inputs/test_predictions.csv``` file. The prediction dataset contains:\n","\n","- **query_id**: query id.\n","- **doc_id**: document id.\n","- **predicted_relevance**: relevance predicted through a ranking algorithm.\n","- **doc_score (y_true)**: actual scores of the documents for the query (ground truth).\n","\n","\n","### 1 - Packages "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEKdVZm5roGY"},"outputs":[],"source":["# you may install any missing package with: \"python3 -m pip install <package_name>\"\n","\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17973,"status":"ok","timestamp":1666261104866,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"-HIpAQAJ_-58","outputId":"7661745b-4284-4202-c487-b7892ffcad52"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"o465EaEQroGZ"},"source":["### 2 - Load data into memory"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1632,"status":"ok","timestamp":1666261106493,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"TEHwgdbEroGZ","outputId":"125dbb6f-1306-4ef8-c661-203eb03ddd15"},"outputs":[],"source":["search_results = pd.read_csv(\"/content/drive/MyDrive/IR_2022_Labs_project/Labs/Lab2-Evaluation/inputs/test_predictions.csv\")\n","search_results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrbhnyBmArFI"},"outputs":[],"source":["#search_results"]},{"cell_type":"markdown","metadata":{"id":"QBdwRd8QroGa"},"source":["Notice that our ground truth consists of multiple levels:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1666261118033,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"D_rhomH8roGa","outputId":"804c5516-1fc9-48c8-d896-08480635f93c"},"outputs":[],"source":["print_result = search_results[\"doc_score\"].unique()\n","print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\".format(len(print_result), sorted(print_result)))"]},{"cell_type":"markdown","metadata":{"id":"52DEN4wOroGb"},"source":["### 3 - Metrics for Ranking\n","\n","#### Binary Relevance\n","\n","To compute *Precision@K, Mean Average Precision* and *Mean Reciprocal Rank*, we need binary relevance (1 = relevant, 0 = not relevant).\n","\n","\n","To simplify the task, we will consider as relevant **all documents that have actual score (doc_score) equal or higher than $2$**, and not-relevant **the remaining documents**.\n","\n","Let's add a column `is_relevant` to our previous table `search_results` following the above rule about **relevance**."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1666261123142,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"2WXb0k7vroGc","outputId":"bc53ec45-7900-4b1e-ffb4-78edf98b0e36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipykernel_10091/3793000711.py\", line 1, in <module>\n","    search_results[\"is_relevant\"] = search_results[\"doc_score\"].apply(lambda y: 1 if y >= 2 else 0)\n","NameError: name 'search_results' is not defined\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 793, in format_exception_as_a_whole\n","    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 839, in get_records\n","    style = stack_data.style_with_executing_node(style, \"bg:ansiyellow\")\n","  File \"/home/alfa/.local/lib/python3.8/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n","    class NewStyle(style):\n","  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n","    ndef[4] = colorformat(styledef[3:])\n","  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n","    assert False, \"wrong color format %r\" % text\n","AssertionError: wrong color format 'ansiyellow'\n"]}],"source":["search_results[\"is_relevant\"] = search_results[\"doc_score\"].apply(lambda y: 1 if y >= 2 else 0)\n","search_results.head()"]},{"cell_type":"markdown","metadata":{"id":"qiALAfwKroGc"},"source":["#### Precision @ K (P@K)\n","\n","Precision at K **(P@K) measures the number of relevant results among the top K documents**. It assesses whether the users are getting relevant documents at the top of the ranking or not.\n","\n","A drawback of this metric is that it fails to take into account the positions of the relevant documents among the top K.\n","\n","Python implementation:\n","Implement the function ```precision_at_k(doc_score, y_score, k)``` that takes as input the true relevance labels, the predicted score, the number of documents to consider K and compute the precision as $k$.\n","\n","Steps:\n","1. use ```np.argsort``` and [::-1] to obtain the list of indexes of the predicted score sorted in descending order.\n","2. use the indexes of point 1 to sort the actual relevance label of the documents (hint: ```np.take```).\n","3. consider the top K relevance label of the documents (after the sorting) and retrieve the number of relevant documents (among the top K, i.e., normalise the number of relevant documents by K).\n","\n","Notice that the P@K is computed for a single query and the respective set of retrieved documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpKoDAZVroGd"},"outputs":[],"source":["def precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    precision @k : float\n","\n","    \"\"\"\n","    order = np.argsort(y_score)[::-1] #we get the ranking of the documents accoirding to the predicted score/ use np.argsort and [::1] to obtain the list of indexes of the predicted score sorted in descending order.\n","    doc_score = np.take(doc_score, order[:k]) # align the binary relevance to the corresponding document / use the indexes of point 1 to sort the actual relevance label of the documents (hint: np.take).\n","    relevant = sum(doc_score == 1) #get number of relevant documents\n","    return float(relevant) / k #calculae precision at k, which is the number of relevant documents trieved at k"]},{"cell_type":"markdown","metadata":{"id":"WP2eveAaroGd"},"source":["Compute Precision@10 for query with query_id=0:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1666261163044,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"8C8JUMI6roGd","outputId":"3d9eba39-deb4-4c78-9f52-a23618eec922"},"outputs":[],"source":["# Check for query 0\n","#P@5= 3/5\n","current_query = 0\n","current_query_res = search_results[search_results[\"query_id\"] == current_query]\n","\n","k = 5\n","print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n","print(\"\\nCheck on the dataset sorted by score:\\n\")\n","#current_query_res.sort_values(\"score\", ascending=False).head(k)\n","current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1666019591969,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"ZiGZA1sProGe","outputId":"9f4ee544-b847-423f-8c88-684f13159cd8"},"outputs":[],"source":["k = 3\n","print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n","\n","k = 10\n","print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9SgfYMt7roGe"},"source":["#### Average Precision@K - AP@K\n","\n","With respect to $P@K$, $AP@K$ gives a better intuition of the model ability to sort the results for a specific query. It tells how much the relevant documents are concentrated in the highest ranked predictions.\n","\n","The Average Precision approximates the area under the un-interpolated precision-recall curve.\n","\n","$$AP@K=\\frac{1}{GTP}\\sum_k^n{P@K \\times rel@K}\\tag{1}$$\n","\n","where: \n","- GTP is the total number of ground truth positives;\n","- P@K is the Precision at K ranked results.\n","- rel@K is a relevance function; it retrieves 1 if the document at rank K is relevant or 0 otherwise.\n","\n","<img src=\"images/apk.png\" style=\"width:600px;height:250px;\">\n","<caption><center> <u> <font color=''> Figure 1 </u><font color=''>  : Computation of AP <br> (Picture taken from <i>https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52</i>)</center></caption>  "]},{"cell_type":"markdown","metadata":{"id":"Ymnt_inlroGe"},"source":["Python implementation:\n","Implement the function ```avg_precision_at_k(doc_score, y_score, k)``` that computes the average precision at K. The function takes as inputs the true relevance labels, the predicted score, and the number of documents to consider K.\n","\n","Notice that the Precision@K is computed for a single query and the respective set of retrieved documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25Vz85MfroGf"},"outputs":[],"source":["def avg_precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    average precision @k : float\n","    \"\"\"\n","    gtp = np.sum(doc_score == 1) #Total number of gt positives\n","    order = np.argsort(y_score)[::-1] #same as for precision\n","    doc_score = np.take(doc_score, order[:k]) #same as for precision\n","    ## if all documents are not relevant\n","    if gtp == 0:\n","        return 0\n","    n_relevant_at_i = 0\n","    prec_at_i = 0\n","    for i in range(len(doc_score)):\n","        if doc_score[i] == 1: #only add the P@k when the doc is relevant\n","            n_relevant_at_i += 1\n","            prec_at_i += n_relevant_at_i / (i + 1) #calculate P@K (#docs relevant at k/k)\n","    return prec_at_i / gtp #return ap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bM5_hdk1roGf"},"source":["Compute Precision@10 for the query with query_id=0:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666261169922,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"_uXNZ3rwroGf","outputId":"4ffdca58-816a-48a9-883c-3c061d452b5d"},"outputs":[],"source":["avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 10)\n","#avg_precision_at_k(np.array([1,0,0,1,1,0]), np.array([0.9,0.8,0.7,0.6,0.5, 0.4]),6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1666261175476,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"48-M-IUZroGf","outputId":"f5973491-d2d7-4244-dda4-e93465382a2d"},"outputs":[],"source":["# Check with 'average_precision_score' of 'sklearn' library\n","\n","from sklearn.metrics import average_precision_score\n","\n","k = 150\n","temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n","average_precision_score(np.array(temp[\"is_relevant\"]), np.array(temp[\"predicted_relevance\"][:k]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6J0tAQtroGf"},"outputs":[],"source":["# Check with 'average_precision_score' of 'sklearn' library\n","\n","doc_score = np.array([1, 1, 0, 1, 0, 0, 1])\n","y_scores = np.array([7, 6, 5, 4, 3, 2, 1])\n","assert (average_precision_score(doc_score, y_scores) == avg_precision_at_k(doc_score, y_scores, 10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzNMvQgmroGf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9iksfC8xroGf"},"source":["Manual check:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666261183004,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"R1nrupAsroGg","outputId":"4dcc4a23-0c7a-4ace-8ae6-39847ec447b3"},"outputs":[],"source":["avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1666261186353,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"cIbQlcV-roGg","outputId":"de350387-d2b8-4a13-f486-6920aac3c030"},"outputs":[],"source":["current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451,"status":"ok","timestamp":1666261229855,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"JRDkB25a6ExK","outputId":"0ce3a234-2fd8-4cda-af14-72a85c119e07"},"outputs":[],"source":["np.sum(current_query_res[\"is_relevant\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1666264475461,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"DNqTta7JroGg","outputId":"95814383-a33b-44a2-e1df-3f76d6ca7038"},"outputs":[],"source":["\n","(1 + (2 / 2) + (3 / 5) + (4 / 7) + (5 / 8) + (6 / 9)) / np.sum(current_query_res[\"is_relevant\"])"]},{"cell_type":"markdown","metadata":{"id":"LJJn9-AoroGg"},"source":["#### Mean Average Precision (mAP)\n","\n","The Mean Average Precision (mAP) is simply the **mean** of all the queries AP. This metric is not computed for a single query as previous metrics, but it takes into account all the queries.\n","\n","Above we mentioned that the average precision approximates the area under the un-interpolated precision-recall curve for a single query. As a consequence, the mAP is roughly the average area under the precision-recall curve for a set of queries.\n","\n","$$mAP=\\frac{1}{N}\\sum_{i=1}^n{AP_i}\\tag{2}$$\n","\n","<img src=\"images/map.png\" style=\"width:600px;height:450px;\">\n","<caption><center> <u> <font color=''> Figure 2 </u><font color=''>  : Computation of mAP </center></caption>  \n","\n","Implement a function ```map_at_k(search_res, k)``` that takes as input the dataset containing search results (list of actual labels, list of predicted scores, list queries) and k, and compute the Mean Average Precision (mAP)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwTlZTVTroGg"},"outputs":[],"source":["def map_at_k(search_res, k=10): #receives all the search esults dataframe containing all the queries and the results and relevances\n","    \"\"\"\n","    Parameters\n","    ----------\n","    search_res: search results dataset containing:\n","        query_id: query id.\n","        doc_id: document id.\n","        predicted_relevance: relevance predicted through LightGBM.\n","        doc_score: actual score of the document for the query (ground truth).\n","\n","    Returns\n","    -------\n","    mean average precision @ k : float\n","    \"\"\"\n","    avp = []\n","    for q in search_res[\"query_id\"].unique():  # loop over all query ids\n","        curr_data = search_res[search_res[\"query_id\"] == q]  # select data for current query (get a slice of the dataframe keeping only the data related to the current query)\n","        avp.append(avg_precision_at_k(np.array(curr_data[\"is_relevant\"]), \n","                   np.array(curr_data[\"predicted_relevance\"]), k))  #append average precision for current query\n","    return np.sum(avp) / len(avp), avp  # return mean average precision"]},{"cell_type":"markdown","metadata":{"id":"_4Y7mDKXroGg"},"source":["Compute mAP@10 for all queries of the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1172,"status":"ok","timestamp":1666264499256,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"yPOMzVbProGh","outputId":"1cb06dfd-6caa-4055-ddef-5dac22fb58c0"},"outputs":[],"source":["map_k, avp = map_at_k(search_results, 10)\n","map_k"]},{"cell_type":"markdown","metadata":{"id":"zaLb4oouroGh"},"source":["#### Mean Reciprocal Rank (MRR)\n","\n","Mean Reciprocal Rank is particularly used when we are interested in 'the first' correct answer.\n","\n","If we define:\n","\n","- $R_i$ as the ranking for the query $q_i$;\n","- $S_{correct}(R_i)$ as the position of the first correct answer in $R_i$\n","- $K$ as the threshold for ranking position\n","\n","The reciprocal rank $$RR(R_i)$$ for query $q_i$ is computed as follows:\n","\n","$$\\begin{equation}\n","  RR(R_i)==\\left\\{\n","  \\begin{array}{@{}ll@{}}\n","    \\frac{1}{S_{correct}(R_i)}, & \\text{if}\\ S_{correct}(R_i) $\\leq$ K \\\\\n","    0, & \\text{otherwise}\n","  \\end{array}\\right.\n","  \\tag{3}\n","\\end{equation} \n","$$\n","\n","\n","The Mean Reciprocal Rank (MRR) can be defined as the mean of the RR for all queries:\n","\n","$$\\begin{equation}\n","  MRR(R_i)==\\frac{1}{N}\\sum_{i=1}^N{RR(R_i)}\n","\\end{equation} \n","\\tag{4}\n","$$\n","\n","where $N$ is the total number of queries (and rankings since we have a ranking per query).\n","\n","**Exercise:**  \n","Implement the function ```rr_at_k(doc_score, y_score, k)``` that computes the Reciprocal Rank at the threshold $k$ for a single query and then compute the MRR@K for k=3, 5 and 10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alIWsM2WroGh"},"outputs":[],"source":["def rr_at_k(doc_score, y_score, k=10):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    Reciprocal Rank for qurrent query\n","    \"\"\"\n","\n","    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order. As before\n","    doc_score = np.take(doc_score, order[\n","                             :k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k. As before\n","    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n","        return 0\n","    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\" (+1 because the idex starts from 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1666264506999,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"LGZV1sYcroGh","outputId":"cc1b79ca-f34b-4cce-c878-85fcdb29f3ba"},"outputs":[],"source":["doc_score = np.array([0, 1, 0, 1, 1])\n","score = np.array([0.9, 0.5, 0.6, 0.7, 0.2])\n","rr_at_k(doc_score, score, 5)"]},{"cell_type":"markdown","metadata":{"id":"ENgNIgmtroGh"},"source":["##### Make some test with the query with query_id = 8 to check if your function is working properly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1666264530681,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"pigWNcSOroGh","outputId":"e53c70d4-0394-404c-b333-38902808b765"},"outputs":[],"source":["current_query = 8\n","current_query_res = search_results[search_results[\"query_id\"] == current_query]\n","current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666019594126,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"k-Gn3o-aroGh","outputId":"557bcdf8-8051-47ed-ea48-92a7dacaf55b"},"outputs":[],"source":["labels = np.array(search_results[search_results['query_id'] == 8][\"is_relevant\"])\n","scores = np.array(search_results[search_results['query_id'] == 8][\"predicted_relevance\"])\n","np.round(rr_at_k(labels, scores, 10), 4)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GprbCLgjroGh"},"source":["##### Compute the MRR@K for k=3,5,and 10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nK4xkjg1roGi"},"outputs":[],"source":["mrr = {}\n","for k in [3, 5, 10]:\n","    RRs = []\n","    for q in search_results['query_id'].unique():  # loop over all query ids, get rrs for each query at each k\n","        labels = np.array(search_results[search_results['query_id'] == q][\"is_relevant\"])  # get labels for current query\n","        scores = np.array(search_results[search_results['query_id'] == q][\"predicted_relevance\"])  # get predicted score for current query\n","        RRs.append(rr_at_k(labels, scores, k))  # append RR for current query\n","    mrr[k] = np.round(float(sum(RRs) / len(RRs)), 4)  # Mean RR at current k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1666264542206,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"wG5g4EsDroGi","outputId":"df6c5076-ce29-43fd-dc26-e283d9d0b142"},"outputs":[],"source":["mrr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vuo9AZyroGi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8mkjVz67roGi"},"source":["### Multiple levels of relevance metrics\n","\n","#### NDCG - Normalized Discounted Cumulative Gain\n","\n","**NDCG** also works if documents relevance are a real number, i.e., when each document's relevance is not expressed in a binary form (relevant or non-relevant).\n","\n","This metric is especially used with machine learning based approaches, like 'Learning To Rank', and it takes values between $0$ (very poor/bad ranking) and $1$ (optimal ranking).\n","\n","Before defining $NDCG$, let's talk about **Discounted Cumulative Gain (DCG)**:\n","**DCG** is based on the following assumptions:\n","- Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n","- Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.\n","\n","$DCG$ is based on the notion of **Cumulative Gain (CG)**:\n","**CG** does not include the position of a result in the consideration of the usefulness of a result set. It is the sum of the relevance values of all results in a search result list (in the ranking). Suppose you were presented with a set of search results for a query and asked to rank each result:\n","- 0 => Not relevant \n","- 1 => Near relevant \n","- 2 => Relevant\n","\n","If we sum the values for a page of results we will have a measure of the cumulative gain (CG).\n","\n","$$CG = \\sum_{pos=1}^n Rel_{pos}\\tag{5}$$\n","\n","Where $Rel_{pos}$ is the graded relevance of $pos^{th}$ document.\n","\n","Cumulative gain, however, does not reward relevant results that appear higher in the result set (CG function is unaffected by changes in the ordering of search results). To achieve the Discounted Cumulative Gain (DCG) we must discount results that appear lower.\n","\n","**Discounted Cumulative Gain (DCG):** The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n","\n","$$DCG = \\sum_{pos=1}^n \\frac{Rel_{pos}}{\\log_2(pos+1)} = Rel_1 + \\sum_{pos=2}^n \\frac{Rel_{pos}}{\\log_2(pos+1)}\\tag{6}$$\n","\n","An alternative formulation of $DCG$ that places stronger emphasis on retrieving relevant documents is the following:\n","\n","$$DCG = \\sum_{pos=1}^n \\frac{2^{Rel_{pos}} -1}{\\log_2(pos+1)}\\tag{7}$$\n","\n","The later formula is commonly used in industry including major web search companies. These two formulations of DCG are the same when the relevance values of documents are binary.\n","\n","**Normalized DCG (NDCG):** If you calculate DCG for different queries you‚Äôll find that some queries are just harder than others and will produce lower DCG scores than easier queries. Normalization solves this problem by scaling the results based off of the best result seen. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position $n$, also called **Ideal DCG (IDCG)** through that position (*usually it is the ground truth*).\n","\n","$$NDCG_{pos} = \\frac{DCG_{pos}}{iDCG}\\tag{8}$$\n","\n","<img src=\"images/ndcg.png\" style=\"width:650px;height:450px;\">\n","<caption><center> <u> <font color=''> Figure 2 </u><font color=''> : Computation of NDCG </br> \n","    (Picture taken from https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832)</center></caption>  "]},{"cell_type":"markdown","metadata":{"id":"lD5hxYgyroGi"},"source":["Implement the functions: \n","- ```dcg_at_k(y_score, doc_score, k)``` based on formula $7$  (approach that makes emphasis in retrieving relevant documents)\n","- ```ndcg_at_k(y_score, doc_score, k)``` Normalized DCG, which uses the ideal DCG\n","\n","Compute:\n","- the $NDCG@10$ for query with ```query_id=0```\n","- the average $NDCG@10$ (considering all queries/rankings)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcuzmjN7roGi"},"outputs":[],"source":["def dcg_at_k(doc_score, y_score, k=10): #doc_scire are the labels (ground truth) and y_score are the system scores\n","    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n","    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n","    gain = 2 ** doc_score - 1  # First we calculate the upper part of the formula which is the CG (use formula 7 above) (notice it is based on the ground truth relevance)\n","    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator (np.arrange creates a list of numbers betweeen 0 and len(doc_score)-1), then the + 2 addresses the fact that the numbers start from 0\n","    return np.sum(gain / discounts)  #return dcg@k\n","\n","\n","def ndcg_at_k(doc_score, y_score, k=10):\n","    dcg_max = dcg_at_k(doc_score, doc_score, k) #ideal dcg\n","    #print(dcg_max)\n","    if not dcg_max:\n","        return 0\n","    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"]},{"cell_type":"markdown","metadata":{"id":"8-4-c0wZroGi"},"source":["##### the  ùëÅùê∑ùê∂ùê∫@10 for query with query_id=0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1666264824048,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"_DxJLrcQroGi","outputId":"d28f2419-4122-457e-e362-84350c8b436e"},"outputs":[],"source":["query_id = 0\n","k = 10\n","labels = np.array(search_results[search_results['query_id'] == query_id][\"doc_score\"])\n","scores = np.array(search_results[search_results['query_id'] == query_id][\"predicted_relevance\"])\n","ndcg_k = np.round(ndcg_at_k(labels, scores, k), 4)\n","print(\"ndcg@{} for query with query_id={}: {}\".format(k, query_id, ndcg_k))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ou0rQNsUroGi"},"source":["##### the average  ùëÅùê∑ùê∂ùê∫@10  (considering all queries/rankings)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2757,"status":"ok","timestamp":1666019605957,"user":{"displayName":"DIANA RAMIREZ","userId":"04589314938991950248"},"user_tz":-120},"id":"WSWwZjCFroGi","outputId":"4b003a81-0284-4177-fe3a-16292ae2e353"},"outputs":[],"source":["ndcgs = []\n","k = 10\n","for q in search_results['query_id'].unique(): # loop over all query ids\n","    labels = np.array(search_results[search_results['query_id'] == q][\"doc_score\"]) ## get labels for current query\n","    scores = np.array(search_results[search_results['query_id'] == q][\"predicted_relevance\"]) # get predicted score for current query\n","    ndcgs.append(np.round(ndcg_at_k(labels, scores, k), 4)) # append NDCG for current query (round is just about decimals)\n","\n","avg_ndcg = np.round(float(sum(ndcgs) / len(ndcgs)), 4)\n","print(\"Average ndcg@{}: {}\".format(k, avg_ndcg))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUcpJA6uroGi"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
