{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lucho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': 'Our list has expanded: \\nCommunity foundations across #Florida have set up relief funds to help those impacted by #HurricaneIan, take action now and share.\\n@CFSarasota\\n@ManateeCF\\n@CollierCFFL \\n@GiveCF\\n@MiamiFoundation\\nhttps://t.co/0uUi16FITA', 'username': 'Community Foundation Awareness Initiative', 'date': 'Fri Sep 30 17:51:40 +0000 2022', 'hashtag': [{'text': 'Florida', 'indices': [53, 61]}, {'text': 'HurricaneIan', 'indices': [113, 126]}], 'like': 2, 'rt': 2, 'URL': None}\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'data/tw_hurricane_data.json'\n",
    "tweets = []\n",
    "lines = []\n",
    "\n",
    "for line in open(docs_path, 'r'):\n",
    "    lines.append(line)\n",
    "    #media = json.loads(line).get('entities').get('media')\n",
    "    tweets.append({\n",
    "        'tweet': json.loads(line).get('full_text'),\n",
    "        'username' : json.loads(line).get('user').get('name'),\n",
    "        'date' : json.loads(line).get('created_at'),\n",
    "        'hashtag' : json.loads(line).get('entities').get('hashtags'), # Si a√±adimos .get('text') obtenemos solo info del hashtag\n",
    "        'like' : json.loads(line).get('favorite_count'),\n",
    "        'rt' : json.loads(line).get('retweet_count'),\n",
    "        'URL' : json.loads(line).get('entities').get('media')\n",
    "    }) #Here we get only the text from the tweets in json document\n",
    "print(tweets[1000])\n",
    "\n",
    "#print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Tweets: {}\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will clean our text from data that is not important so that has no weight \n",
    "def clean_text(tweets):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "   \n",
    "    tweets = tweets.get('tweet').lower() # Transform in lowercase\n",
    "    tweets = re.sub(r'[^\\w\\s]', '', tweets) # Here we remove punctuation marks\n",
    "    tweets = tweets.split() # Tokenize the text to get a list of terms\n",
    "    tweets = [tweet for tweet in tweets if tweet not in stop_words]  # Eliminate the stopwords \n",
    "    tweets = [stemmer.stem(tweet) for tweet in tweets] # Perform stemming \n",
    "    ## END CODE    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'expand', 'commun', 'foundat', 'across', 'florida', 'set', 'relief', 'fund', 'help', 'impact', 'hurricaneian', 'take', 'action', 'share', 'cfsarasota', 'manateecf', 'colliercffl', 'givecf', 'miamifound', 'httpstco0uui16fita']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(tweets[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "\n",
    "    current_page_index = {}\n",
    "    title_index = {}\n",
    "    for i in range(len(tweets)):\n",
    "        #tw_arr = tweets[i].split(\"|\")\n",
    "        terms = clean_text(tweets[i])\n",
    "        page_id = terms[0]\n",
    "        title = terms[1]\n",
    "        title_index[page_id]=title\n",
    "    for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "        try:\n",
    "            # if the term is already in the index for the current page (current_page_index)\n",
    "            # append the position to the corresponding list\n",
    "\n",
    "    ## START CODE\n",
    "            current_page_index[term].append(position)  #1 porque lo que queremos llenar es la sublista\n",
    "        except:\n",
    "            # Add the new term as dict key and initialize the array of positions and add the position\n",
    "            current_page_index[term]=[tweet, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "    #merge the current page index with the main index\n",
    "    for term_page, posting_page in current_page_index.items():\n",
    "        index[term_page].append(posting_page)\n",
    "\n",
    "    ## END CODE                    \n",
    "\n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "index,title_index = create_index(tweets)\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['research'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-3b3465fd49e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-156-3c4583b74a49>\u001b[0m in \u001b[0;36mcreate_index\u001b[1;34m(tweets)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Remember, lines contain all documents: article-id | article-title | article-body\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtw_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mpage_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtw_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtw_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# page_title + page_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'tweet'"
     ]
    }
   ],
   "source": [
    "create_index(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
