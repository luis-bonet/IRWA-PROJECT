{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alfa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import time \n",
    "import json\n",
    "import re\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will clean our text from data that is not important so that has no weight \n",
    "def clean_text(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    tweet = tweet.lower() # Transform in lowercase\n",
    "\n",
    "    tweet = re.sub(r'@[a-zA-Z]+', '', tweet) # Here we remove the mentions in the tweet ex: @canodep\n",
    "    tweet = re.sub(r\"\\B#([a-z0-9]{2,})(?![~!@#$%^&*()=+_`\\-\\|\\/'\\[\\]\\{\\}]|[?.,]*\\w)\", '', tweet) # Here we remove the hashtags, because we will treat it later\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Here we remove punctuation marks\n",
    "    tweet = re.sub(r'http\\S+', '',tweet) # Remove http and https\n",
    "    tweet = tweet.split() # Tokenize the text to get a list of terms\n",
    "\n",
    "    tweet = [word for word in tweet if word not in stop_words]  # eliminate the stopwords\n",
    "    tweet = [stemmer.stem(word) for word in tweet] # Perform stemming \n",
    "    return tweet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = 'data/tw_hurricane_data.json'\n",
    "tweets_title = 'data/tweet_document_ids_map.csv'\n",
    "\n",
    "tweets_id_title = {}\n",
    "\n",
    "with open(tweets_title) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "\n",
    "for l in lines:\n",
    "    l = l.strip().split(\"\\t\")\n",
    "    tweets_id_title[int(l[1])] =  l[0]\n",
    "\n",
    "\n",
    "tweets = []\n",
    "lines = []\n",
    "\n",
    "for line in open(docs_path, 'r'):\n",
    "    lines.append(line)\n",
    "    #media = json.loads(line).get('entities').get('media')\n",
    "    tweets.append({\n",
    "        'id' : int(json.loads(line).get('id')),\n",
    "        'title' : tweets_id_title[int(json.loads(line).get('id'))],\n",
    "        'text': clean_text(json.loads(line).get('full_text')),\n",
    "        'username' : json.loads(line).get('user').get('screen_name'),\n",
    "        'date' : json.loads(line).get('created_at'),\n",
    "        'hashtag' : list(map(lambda hashtag:  hashtag.get('text'),  json.loads(line).get('entities').get('hashtags'))),\n",
    "        'like' : json.loads(line).get('favorite_count'),\n",
    "        'rt' : json.loads(line).get('retweet_count'),\n",
    "        'URL' : 'https://twitter.com/' + json.loads(line).get('user').get('screen_name') + \"/status/\" + str(json.loads(line).get('id'))\n",
    "    }) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)    \n",
    "\n",
    "    for i in range(len(tweets)):\n",
    "        terms = tweets[i].get(\"text\")\n",
    "        tweet_id = tweets[i].get('id')\n",
    "\n",
    "        tweet_title = tweets_id_title[tweet_id]\n",
    "\n",
    "        current_tweet_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "\n",
    "                ## START CODE\n",
    "                current_tweet_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_tweet_index[term] = [tweet_title, array('I', [position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term, posting_page in current_tweet_index.items():\n",
    "            index[term].append(posting_page)\n",
    "        ## END CODE                    \n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[['doc_1', array('I', [0, 1])], ['doc_220', array('I', [8])], ['doc_405', array('I', [8])], ['doc_1354', array('I', [1])], ['doc_1612', array('I', [2])], ['doc_2026', array('I', [6])], ['doc_2600', array('I', [9])], ['doc_2748', array('I', [11])], ['doc_3132', array('I', [2])], ['doc_3307', array('I', [5])]]\n"
     ]
    }
   ],
   "source": [
    "index = create_index(tweets)\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['research'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = clean_text(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"\n",
    "            term_docs = [posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 342 for the searched query:\n",
      "\n",
      "page_id= 1575860716518535168 - page_title: doc_3662\n",
      "page_id= 1575905160944943104 - page_title: doc_1070\n",
      "page_id= 1575863343080157185 - page_title: doc_3488\n",
      "page_id= 1575901851844845568 - page_title: doc_1277\n",
      "page_id= 1575899535934488581 - page_title: doc_1406\n",
      "page_id= 1575872675611783168 - page_title: doc_2723\n",
      "page_id= 1575906817322123264 - page_title: doc_973\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575910656234074112 - page_title: doc_607\n",
      "page_id= 1575880812318584832 - page_title: doc_2187\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 819 for the searched query:\n",
      "\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575907837544275969 - page_title: doc_894\n",
      "page_id= 1575889512118353921 - page_title: doc_1800\n",
      "page_id= 1575856395072770048 - page_title: doc_3982\n",
      "page_id= 1575902735710375938 - page_title: doc_1213\n",
      "page_id= 1575914899251040256 - page_title: doc_257\n",
      "page_id= 1575912364842418176 - page_title: doc_462\n",
      "page_id= 1575874057383612417 - page_title: doc_2629\n",
      "page_id= 1575864114693672961 - page_title: doc_3429\n",
      "page_id= 1575908873197670401 - page_title: doc_781\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 485 for the searched query:\n",
      "\n",
      "page_id= 1575881179907112961 - page_title: doc_2173\n",
      "page_id= 1575886896194281472 - page_title: doc_1914\n",
      "page_id= 1575913610983133184 - page_title: doc_375\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575877082847903744 - page_title: doc_2391\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575885745646075906 - page_title: doc_1959\n",
      "page_id= 1575864114693672961 - page_title: doc_3429\n",
      "page_id= 1575869514004119555 - page_title: doc_2993\n",
      "page_id= 1575913693207937024 - page_title: doc_365\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 1072 for the searched query:\n",
      "\n",
      "page_id= 1575870975328935936 - page_title: doc_2847\n",
      "page_id= 1575907837544275969 - page_title: doc_894\n",
      "page_id= 1575902735710375938 - page_title: doc_1213\n",
      "page_id= 1575917865743663125 - page_title: doc_23\n",
      "page_id= 1575897959727828992 - page_title: doc_1454\n",
      "page_id= 1575880612338372610 - page_title: doc_2196\n",
      "page_id= 1575917153777483776 - page_title: doc_67\n",
      "page_id= 1575883508060299265 - page_title: doc_2058\n",
      "page_id= 1575913266819497984 - page_title: doc_396\n",
      "page_id= 1575895171799158785 - page_title: doc_1558\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 576 for the searched query:\n",
      "\n",
      "page_id= 1575881179907112961 - page_title: doc_2173\n",
      "page_id= 1575886896194281472 - page_title: doc_1914\n",
      "page_id= 1575863343080157185 - page_title: doc_3488\n",
      "page_id= 1575913610983133184 - page_title: doc_375\n",
      "page_id= 1575877082847903744 - page_title: doc_2391\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575901851844845568 - page_title: doc_1277\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575880812318584832 - page_title: doc_2187\n",
      "page_id= 1575885745646075906 - page_title: doc_1959\n"
     ]
    }
   ],
   "source": [
    "#print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "#query = input()\n",
    "#docs = search(query, index)\n",
    "#top = 10\n",
    "\n",
    "#print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "#for d_id in docs[:top]:\n",
    "#    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "\n",
    "query = \"My house floods\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "\n",
    "query = \"I am scared to death, a hurricane is coming to my city\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "query = \"Landfall in South Carolina\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "query = \"Help and recovery during the hurricane disaster\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "query = \"Floodings in South Carolina\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 342 for the searched query:\n",
      "\n",
      "page_id= 1575860716518535168 - page_title: doc_3662\n",
      "page_id= 1575905160944943104 - page_title: doc_1070\n",
      "page_id= 1575863343080157185 - page_title: doc_3488\n",
      "page_id= 1575901851844845568 - page_title: doc_1277\n",
      "page_id= 1575899535934488581 - page_title: doc_1406\n",
      "page_id= 1575872675611783168 - page_title: doc_2723\n",
      "page_id= 1575906817322123264 - page_title: doc_973\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575910656234074112 - page_title: doc_607\n",
      "page_id= 1575880812318584832 - page_title: doc_2187\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 819 for the searched query:\n",
      "\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575907837544275969 - page_title: doc_894\n",
      "page_id= 1575889512118353921 - page_title: doc_1800\n",
      "page_id= 1575856395072770048 - page_title: doc_3982\n",
      "page_id= 1575902735710375938 - page_title: doc_1213\n",
      "page_id= 1575914899251040256 - page_title: doc_257\n",
      "page_id= 1575912364842418176 - page_title: doc_462\n",
      "page_id= 1575874057383612417 - page_title: doc_2629\n",
      "page_id= 1575864114693672961 - page_title: doc_3429\n",
      "page_id= 1575908873197670401 - page_title: doc_781\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 485 for the searched query:\n",
      "\n",
      "page_id= 1575881179907112961 - page_title: doc_2173\n",
      "page_id= 1575886896194281472 - page_title: doc_1914\n",
      "page_id= 1575913610983133184 - page_title: doc_375\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575877082847903744 - page_title: doc_2391\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575885745646075906 - page_title: doc_1959\n",
      "page_id= 1575864114693672961 - page_title: doc_3429\n",
      "page_id= 1575869514004119555 - page_title: doc_2993\n",
      "page_id= 1575913693207937024 - page_title: doc_365\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 1072 for the searched query:\n",
      "\n",
      "page_id= 1575870975328935936 - page_title: doc_2847\n",
      "page_id= 1575907837544275969 - page_title: doc_894\n",
      "page_id= 1575902735710375938 - page_title: doc_1213\n",
      "page_id= 1575917865743663125 - page_title: doc_23\n",
      "page_id= 1575897959727828992 - page_title: doc_1454\n",
      "page_id= 1575880612338372610 - page_title: doc_2196\n",
      "page_id= 1575917153777483776 - page_title: doc_67\n",
      "page_id= 1575883508060299265 - page_title: doc_2058\n",
      "page_id= 1575913266819497984 - page_title: doc_396\n",
      "page_id= 1575895171799158785 - page_title: doc_1558\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 576 for the searched query:\n",
      "\n",
      "page_id= 1575881179907112961 - page_title: doc_2173\n",
      "page_id= 1575886896194281472 - page_title: doc_1914\n",
      "page_id= 1575863343080157185 - page_title: doc_3488\n",
      "page_id= 1575913610983133184 - page_title: doc_375\n",
      "page_id= 1575877082847903744 - page_title: doc_2391\n",
      "page_id= 1575872350159077376 - page_title: doc_2747\n",
      "page_id= 1575901851844845568 - page_title: doc_1277\n",
      "page_id= 1575898304386568192 - page_title: doc_1447\n",
      "page_id= 1575880812318584832 - page_title: doc_2187\n",
      "page_id= 1575885745646075906 - page_title: doc_1959\n"
     ]
    }
   ],
   "source": [
    "#print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "#query = input()\n",
    "#docs = search(query, index)\n",
    "#top = 10\n",
    "\n",
    "#print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "#for d_id in docs[:top]:\n",
    "#    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "\n",
    "query = \"My house floods\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "\n",
    "query = \"I am scared to death, a hurricane is coming to my city\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "query = \"Landfall in South Carolina\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "query = \"Help and recovery during the hurricane disaster\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "\n",
    "\n",
    "query = \"Floodings in South Carolina\"\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(list(tweets_id_title.keys())[list(tweets_id_title.values()).index(d_id)], d_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, num_docs):\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  # term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  # document frequencies of terms in the corpus\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet_id = tweet['id']\n",
    "        terms = tweet['text']\n",
    "        title = tweet['title']\n",
    "        title_index[tweet_id] = title\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                current_page_index[term] = [tweet_id, array('I', [position])]\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        for term, posting in current_page_index.items():\n",
    "            tf[term].append((tweet_id, np.round(len(posting[1])) / norm, 4))\n",
    "            df[term] = 1\n",
    "\n",
    "        for term, posting in current_page_index.items():\n",
    "            index[term].append(posting)\n",
    "\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_docs / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 88.81 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(tweets)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(tweets, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    terms = clean_text(query)\n",
    "    docs = set()\n",
    "    for term in terms:\n",
    "        try:\n",
    "            term_docs= [posting[0] for posting in index[term]]\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return rank_documents(terms, docs, index, idf, tf, title_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    query_terms_count = collections.Counter(terms)\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = idf[term] * tf[term][doc_index][1]\n",
    "\n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [doc for score, doc in doc_scores]\n",
    "\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No documents found for the given query!\")\n",
    "        query = input()\n",
    "        result_docs = search_tf_idf(query, index)\n",
    "\n",
    "    return doc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 385 for the searched query:\n",
      "\n",
      "tweet_id= 1575897763736330242 - page_title: doc_1458, doc_score: 52.00068178162995\n",
      "tweet_id= 1575912174131630080 - page_title: doc_477, doc_score: 48.642183776379206\n",
      "tweet_id= 1575863229548752897 - page_title: doc_3495, doc_score: 48.642183776379206\n",
      "tweet_id= 1575900431573651456 - page_title: doc_1374, doc_score: 45.86029066666667\n",
      "tweet_id= 1575892486484017155 - page_title: doc_1679, doc_score: 45.86029066666667\n",
      "tweet_id= 1575869807953592324 - page_title: doc_2964, doc_score: 41.48219370490119\n",
      "tweet_id= 1575901596680359938 - page_title: doc_1301, doc_score: 39.71617674227173\n",
      "tweet_id= 1575884103056080898 - page_title: doc_2027, doc_score: 39.71617674227173\n",
      "tweet_id= 1575856919931105280 - page_title: doc_3952, doc_score: 39.71617674227173\n",
      "tweet_id= 1575862439358631936 - page_title: doc_3560, doc_score: 38.15806834761907\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "        print(\"tweet_id= {} - page_title: {}, doc_score: {}\".format(d_id[1], tweets_id_title[d_id[1]], d_id[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Rank-Based Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  query_id  label\n",
       "0   doc_12         1      1\n",
       "1    doc_9         1      1\n",
       "2   doc_18         1      1\n",
       "3   doc_45         1      1\n",
       "4  doc_501         1      1"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.read_csv(\"data/evaluation_gt.csv\")\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground truth of our dataset is composed of 2 Relevance Levels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print_result = search_results[\"label\"].unique()\n",
    "print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\".format(len(print_result), sorted(print_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[\"is_relevant\"] = search_results[\"label\"].apply(lambda y: 1 if y >= 1 else 0)\n",
    "search_results.head()\n",
    "\n",
    "doc_scores = search_tf_idf(\"Landfall in South Carolina\", index)\n",
    "result_docs = []\n",
    "for score, doc in doc_scores:\n",
    "    if(doc in tweets_id_title.keys()):\n",
    "        result_docs.append([tweets_id_title[doc], score])\n",
    "\n",
    "results_df = pd.DataFrame(result_docs, columns=[\"doc_title\", \"predicted_relevance\"])\n",
    "search_results = pd.merge(search_results, results_df ,left_on='doc',right_on='doc_title', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    order = np.argsort(y_score)[::-1] #we get the ranking of the documents accoirding to the predicted score/ use np.argsort and [::1] to obtain the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k]) # align the binary relevance to the corresponding document / use the indexes of point 1 to sort the actual relevance label of the documents (hint: np.take).\n",
    "    relevant = sum(doc_score == 1) #get number of relevant documents\n",
    "\n",
    "    return float(relevant) / k #calculae precision at k, which is the number of relevant documents trieved at k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@5: 1.0\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28889/2589672236.py:16: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  doc_score = np.take(doc_score, order[:k]) # align the binary relevance to the corresponding document / use the indexes of point 1 to sort the actual relevance label of the documents (hint: np.take).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>predicted_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_82</td>\n",
       "      <td>39.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_501</td>\n",
       "      <td>34.395218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_165</td>\n",
       "      <td>28.083578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_100</td>\n",
       "      <td>22.125327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_18</td>\n",
       "      <td>21.753446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  query_id  label  is_relevant doc_title  predicted_relevance\n",
       "6   doc_82         1      1            1    doc_82            39.716177\n",
       "4  doc_501         1      1            1   doc_501            34.395218\n",
       "9  doc_165         1      1            1   doc_165            28.083578\n",
       "7  doc_100         1      1            1   doc_100            22.125327\n",
       "2   doc_18         1      1            1    doc_18            21.753446"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P@5= 3/5\n",
    "current_query = 1\n",
    "current_query_res = search_results[search_results[\"query_id\"] == current_query]\n",
    "k = 5\n",
    "print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "#current_query_res.sort_values(\"doc_score\", ascending=False).head(k)\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@3: 1.0\n",
      "\n",
      "==> Precision@10: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28889/2589672236.py:16: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  doc_score = np.take(doc_score, order[:k]) # align the binary relevance to the corresponding document / use the indexes of point 1 to sort the actual relevance label of the documents (hint: np.take).\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n",
    "k = 10\n",
    "print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Precision@K - AP@K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    \"\"\"\n",
    "    gtp = np.sum(doc_score == 1) #Total number of gt positives\n",
    "    order = np.argsort(y_score)[::-1] #same as for precision\n",
    "    doc_score = np.take(doc_score, order[:k]) #same as for precision\n",
    "    ## if all documents are not relevant\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1: #only add the P@k when the doc is relevant\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i + 1) #calculate P@K (#docs relevant at k/k)\n",
    "    return prec_at_i / gtp #return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28889/2175404873.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  average_precision_score(np.array(temp[\"is_relevant\"]), np.array(temp[\"predicted_relevance\"][:k]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check with 'average_precision_score' of 'sklearn' library\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "k = 150\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"is_relevant\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
