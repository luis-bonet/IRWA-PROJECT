{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFXDf0Tb0zWe"
   },
   "source": [
    "# Information Retrieval and Web Analytics\n",
    "\n",
    "## Scraping Twitter Data\n",
    "\n",
    "In this lab exercise we'll work with Twitter data. For that you will need to use your Twitter Developer account (or create a new one):\n",
    "\n",
    "We'll do the following steps:\n",
    "\n",
    "1. Get the API bearer token for authenticating the API connections.\n",
    "\n",
    "2. Learn about the main functions of Twitter API, the typical json structure and the use of Tweepy library for Python.\n",
    "\n",
    "3. Make queries with different parameters.\n",
    "\n",
    "4. Download the user timeline, only the last 3200 tweets.\n",
    "\n",
    "5. Work with the Twitter Streaming API.\n",
    "\n",
    "6. Analyze the received JSON data, and display it.\n",
    "\n",
    "\n",
    "We will not post results to any external website, neither share the data with third parties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fyu9SJe50zWi"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "# from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC2gSizG0zWj"
   },
   "source": [
    "# 1. Getting the API Keys and Tokens\n",
    "\n",
    "Connecting to Twitter API requires to have API keys and tokens. We can get those with a **Twitter Developer Account** by doing the following steps:\n",
    "\n",
    "**1st step** Login into the Twitter Developer Portal with you personal account: [https://developer.twitter.com/en/portal](https://developer.twitter.com/en/portal)\n",
    "\n",
    "**2nd step** Create a Developer Account\n",
    "\n",
    "If this is your first time in the Twitter Developer you will need to Apply for the creation and approval. Select Student as the use case, and choose no regarding the use of the content for a government entity. Then accept the Terms & Conditions.\n",
    "\n",
    "![Figure 1](https://drive.google.com/uc?export=view&id=1bs4kmVEG-sWJMR4R5QjQHSIplxKJYQSw)\n",
    "<center><caption> <u>Figure 1</u>: Creating a developer account</caption></center>\n",
    "\n",
    "**3rd step** Now we **Create an app**.\n",
    "\n",
    "Name the application as \"IR_class_Your_name\"\n",
    "\n",
    "![Figure 2](https://drive.google.com/uc?export=view&id=145FvO5K-aWg0Ad624g53Oojjieivmuqk)\n",
    "<center><caption> <u>Figure 2</u>: Creating an app</caption></center>\n",
    "\n",
    "**4th step** API Credentials (Keys and Tokens)\n",
    "\n",
    "The application we created will have credentials that will allow us to connect to the Twitter API.\n",
    "\n",
    "**API Key**: is like the application username.\n",
    "**API Secret**: is like the application password.\n",
    "**Access Token**: your user access token.\n",
    "**Access Token Secret**: your user secret.\n",
    "\n",
    "These prior keys and tikes are often used to access to the API V1. \n",
    "\n",
    "**Bearer Token**: This will give you access to Twitter API V2, which is the one available for the new accounts created. This is the token we are mostly going to use for this session.\n",
    "\n",
    "Store them in a safe place. We'll also use them below for the exercise.\n",
    "\n",
    "![Figure 3](https://drive.google.com/uc?export=view&id=1HROwx6FBovt1PzTOuBlzlq6Mfn_aSDvg)\n",
    "<center><caption> <u>Figure 3</u>: Getting the keys and tokens to access the API</caption></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8o0crVn0zWk"
   },
   "outputs": [],
   "source": [
    "## Save Tweeter App credentials in variables:\n",
    "## Save Tweeter App credentials in variables:\n",
    "api_key = \"YOUR CODE HERE\"\n",
    "api_secret = \"YOUR CODE HERE\"\n",
    "\n",
    "access_token = \"YOUR CODE HERE\"\n",
    "access_token_secret = \"YOUR CODE HERE\"\n",
    "\n",
    "bearer= \"YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeMG4kA20zWl"
   },
   "outputs": [],
   "source": [
    "# Download Tweepy into current environment.\n",
    "import sys\n",
    "# or use below one for pip install...\n",
    "!pip3 install tweepy --upgrade\n",
    "\n",
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GayzFUg_0zWm"
   },
   "outputs": [],
   "source": [
    "# authenticate with our app credentials:\n",
    "client = tweepy.Client(bearer_token=bearer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiU0Sr8u0zWm"
   },
   "source": [
    "# 2. Introduction to main functions of Twitter API + Tweepy\n",
    "\n",
    "**What is Twitter**\n",
    "\n",
    "Twitter is an online social network, which allows users to post short messages/status, 280-character each tweets. Because of the continuous flood of tweets generated every day, Twitter is a great source to get rapid and current information through the web. It can be particularly useful to understand how people interacts with respect to a specific (current) topic, how they discuss and create conversations (e.g. threads) or just to keep yourself posted about what's happening around the world.\n",
    "\n",
    "**Why Twitter for scraping data**\n",
    "\n",
    "In the last years, with the limitations imposed by law restrictions, accessing social media data through scraping is becoming a challenging task. Nevertheless, Twitter remains (with Reddit) one of the few that is still accessible and relatively open to do research studies and analyze aggregated social network data.\n",
    "\n",
    "**Main functionalities of the Twitter API**\n",
    "\n",
    "Through the Twitter API, any developer can access Twitter data in several ways. It's possible to retrieve users timeline (the last 3200 tweets posted by a single user), it's possible to collect conversation around a topic, up to 7 days before and, as main functionality, it's possible to track the live stream of information related to a specific topic (which can be easily tracked through hashtags).\n",
    "\n",
    "\n",
    "**Python library Tweepy**\n",
    "\n",
    "Tweepy is a popular Python library that helps us to play with the Twitter API.\n",
    "\n",
    "**Some practical examples of studies performed using Twitter Data**\n",
    "\n",
    "Here are some studies performed analyzing Twitter Data:\n",
    "- [Quantifying Controversy in Social Media](https://arxiv.org/abs/1507.05224)\n",
    "- [Falling into the Echo Chamber: The Italian Vaccination Debate on Twitter](https://ojs.aaai.org//index.php/ICWSM/article/view/7285)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qNvF37q0zWn"
   },
   "source": [
    "# 3. Downloading a user timeline\n",
    "Returns a collection of the most recent Tweets posted by the user indicated by the `user_id` parameters.\n",
    "\n",
    "\n",
    "Reference: [https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/introduction](https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/introduction)\n",
    "\n",
    "Tweet object data: [https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOI26rKQ0zWn"
   },
   "outputs": [],
   "source": [
    "user_name = 'canodep'\n",
    "user_id=client.get_user(username=user_name).data.id #first we get the user id using the username as argument\n",
    "tweets = client.get_users_tweets(id=user_id, tweet_fields=['id','text','created_at','public_metrics']) #then we choose the elements we want to retrieve for a given tweet\n",
    "for tweet in tweets.data: #display tweets collected\n",
    "    print(tweet.id, tweet.created_at, tweet.text,  tweet.public_metrics,\"\\n\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9ZUDRq_0zWo"
   },
   "outputs": [],
   "source": [
    "print(\"Number of tweets for \" + user_name + \": \", len(tweets.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_ycCQPjqBQJ"
   },
   "outputs": [],
   "source": [
    "print(\"Printing second tweet (position 1): \")\n",
    "print(tweets.data[1])\n",
    "\n",
    "print(\"Printing first tweet RT Status (position 1): \")\n",
    "\n",
    "print(tweets.data[1].public_metrics)\n",
    "\n",
    "print(\"Retweet Count (position 1):\",tweets.data[1].public_metrics['retweet_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zamnMiae0zWo"
   },
   "source": [
    "**So we get only the last 200? What about all the other tweets?**\n",
    "\n",
    "With the Standard Twitter API we can get up to the last 3200 tweets. If we want more, we have to get premium account, which is quite expensive. In our case we don't really need it, because we mainly work with streaming data, which produces good amount of data in relative short amount of time.\n",
    "\n",
    "**But... How do we get the 3200 tweets for one user?** \n",
    "\n",
    "It's possible to retrieve the last tweets of all the public users (private ones we can't). In the following example we will go through the last 3200 tweets of NASA Hubble Space Telescope (@NASAHubble).\n",
    "\n",
    "\n",
    "Reference: http://docs.tweepy.org/en/latest/cursor_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DamyLeOUeZnQ"
   },
   "source": [
    "1st Approach - Using Paginator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41QaYNoE0zWp"
   },
   "outputs": [],
   "source": [
    "user_name = 'NASAHubble'\n",
    "user_id=client.get_user(username=user_name).data.id #first we get the user id using the username as argument\n",
    "tweets = tweepy.Paginator(client.get_users_tweets,id=user_id, tweet_fields=['id','text','created_at','public_metrics'],\n",
    "                              max_results=100).flatten(limit=3300) #then we define a paginator which will go through each page of results, we give it the function we are calling (get_users_tweets), and its arguments as we did without the paginator (id, tweet_fiels, and amount of tweets to collect per page)\n",
    "\n",
    "#we have to store data as they are brought by the paginator\n",
    "tweets_dic={} #to keep the data\n",
    "for count, tweet in enumerate(tweets):\n",
    "    tweet_num=count+1\n",
    "    print(tweet_num, tweet.id, tweet.text,tweet.created_at,tweet.public_metrics,\"\\n\")   \n",
    "    tweets_dic[tweet_num]={}\n",
    "    tweets_dic[tweet_num][\"id\"]=tweet.text \n",
    "    tweets_dic[tweet_num][\"text\"]=tweet.text \n",
    "    tweets_dic[tweet_num][\"created_at\"]=tweet.created_at \n",
    "    tweets_dic[tweet_num][\"public_metrics\"]=tweet.public_metrics \n",
    "\n",
    "\n",
    "print(\"Number of tweets for \" + user_name + \": \", tweet_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g9Bh9iO0zWp"
   },
   "outputs": [],
   "source": [
    "print(\"Printing second tweet (position 2): \")\n",
    "print(tweets_dic[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5sfRn0BBpQ5"
   },
   "source": [
    "https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "\n",
    "\n",
    "Fields:\n",
    "https://developer.twitter.com/en/docs/twitter-api/fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s6KhPBM0zWq"
   },
   "outputs": [],
   "source": [
    "print(\"Printing first tweet number of retweets received (position 1): \")\n",
    "print(tweets_dic[1]['public_metrics']['retweet_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wC7ggR70zWq"
   },
   "source": [
    "Print the text of the tweet with more likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50jlfH1V0zWq"
   },
   "outputs": [],
   "source": [
    "more_likes = 0\n",
    "text_with_more_likes = \"\"\n",
    "for tweet in tweets_dic:\n",
    "    likes_tweet = tweets_dic[tweet]['public_metrics']['like_count']     #get the like counts in the structure\n",
    "    if likes_tweet > more_likes: #apply the algorithm to get the max value in a list\n",
    "        more_likes = likes_tweet\n",
    "        text_with_more_likes = tweets_dic[tweet][\"text\"]\n",
    "        \n",
    "              \n",
    "print(\"Most liked tweet: {} \\n with a total of {} likes.\".format(text_with_more_likes,more_likes))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-NP4_Af0zWr"
   },
   "source": [
    "Print the text of the tweet with more retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpvcZAAb0zWr"
   },
   "outputs": [],
   "source": [
    "more_retweets = 0\n",
    "text_with_more_retweets = \"\"\n",
    "for tweet in tweets_dic:    \n",
    "    num_retweets =  tweets_dic[tweet]['public_metrics']['retweet_count'] #get the public metrics in the structure\n",
    "    if num_retweets > more_retweets:\n",
    "        more_retweets = num_retweets\n",
    "        text_with_more_retweets = tweets_dic[tweet][\"text\"]\n",
    "print(\"Most retweeted tweet: {} \\n with a total of {} retweets.\".format(text_with_more_retweets,more_retweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRn1jdvR0zWr"
   },
   "source": [
    "# 4 - Working with the Streaming API\n",
    "  \n",
    "The Twitter API allows you to \"listen\" to the public stream of Tweets, collecting its data and filtering them by topic. To do so, we need to set a \"Stream Listener\", through Tweepy, in order to set some parameters like how many tweets to collect, how to store them, how to handle errors, etc.\n",
    "\n",
    "We are going to collect the tweets and save them to a file in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN6FCX4nKBVv"
   },
   "source": [
    "Tweepy reference for streaming: https://docs.tweepy.org/en/stable/streamingclient.html#tweepy.StreamingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv4x6ZHpAmDb"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from progressbar import ProgressBar, Percentage, Bar\n",
    "class MyStream(tweepy.StreamingClient):\n",
    "    # This function gets called when the stream is working\n",
    "     def __init__(self, bearer_tok, output_filename,  stop_condition=10):\n",
    "        \"\"\"\n",
    "        initialize the stream, with num. of tweets and saving the output file\n",
    "        \"\"\"  \n",
    "         \n",
    "        super(MyStream, self).__init__(bearer_tok)\n",
    "        # to-count the number of tweets collected\n",
    "        self.num_tweets = 0\n",
    "\n",
    "        #the token to access the API\n",
    "        self.bearer =bearer\n",
    "\n",
    "        # save filename\n",
    "        self.filename = output_filename\n",
    "\n",
    "        # stop-condition\n",
    "        self.stop_condition = stop_condition       \n",
    "                \n",
    "        self.file = open(self.filename, \"a+\") #Open file to store tweets as we stream\n",
    "\n",
    "        self.pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=self.stop_condition).start() #start a progress bar\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def on_connect(self): #what we do once the connection is estabished\n",
    "        print(\"Connected\")\n",
    "\n",
    "    def on_tweet(self, tweet): #what we do as each tweet is retrieved      \n",
    "        self.num_tweets += 1  #update amount of tweets collected\n",
    "       # Check stop condition\n",
    "        if self.num_tweets <= self.stop_condition:\n",
    "            self.file.write(json.dumps(tweet.data) + '\\n') #store tweet\n",
    "            print('\\nNew Tweet {0}: {1}'.format(self.num_tweets, tweet.text[0:80]))            \n",
    "            self.pbar.update(self.num_tweets) #update progress\n",
    "            time.sleep(0.2) #just for visualization when printing        \n",
    "          \n",
    "        else: #stop streaming\n",
    "            self.pbar.finish()\n",
    "            print(\"Tweets persisted in file \" + self.filename)\n",
    "            self.file.close()\n",
    "            # sys.exit(0) # not for Notebook, use only in local\n",
    "            self.disconnect() \n",
    "       \n",
    "         \n",
    "    def on_error(self, status):\n",
    "        \"\"\"\n",
    "        function useful to handle errors. It's possible to personalize it\n",
    "        depending on the way we want to handle errors\n",
    "        \"\"\"\n",
    "        return False\n",
    "        print(status)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXgZRHii0zWs"
   },
   "source": [
    "We are going to stream tweets addressing a particular topic by giving our streamer a set of keywords to search. We can also search hashtags (#) and users (@).\n",
    "\n",
    "Here we download a bunch of tweets with a set of keywords related to covid ```[\"covid\", \"vaccines\", \"pandemy\"]```\n",
    "\n",
    "Change value of variable `stop_cond` to `1000` and wait several minutes until completion.\n",
    "\n",
    "**This will take a while!** Check the progress bar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4LMBukY0zWs"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_filename = \"one.json\"\n",
    "#os.remove(output_filename) # restart with a fresh file\n",
    "stop_cond = 500 # test with 1000, takes a longer time\n",
    "\n",
    "stream = MyStream(bearer,output_filename,stop_cond) #initializing instance\n",
    "\n",
    "search_terms=[\"covid\",\"vaccines\",\"pandemy\"] #defne list of search terms\n",
    "\n",
    "for term in search_terms:\n",
    "    stream.add_rules(tweepy.StreamRule(term)) #adding the rules for the streamer\n",
    "\n",
    "stream.filter(expansions=\"author_id\", tweet_fields=[\"public_metrics\",\"created_at\",\"author_id\",\"entities\",\"referenced_tweets\"]) #runs the stream. With the expanssions we ask for information that does nit come by default in a tweet object. We can also ask for the tweet fields we want to retrieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oTokuIQ0zWs"
   },
   "source": [
    "# 5. Let's analyze the json data\n",
    "\n",
    "**A. Statistical description of data**\n",
    "    \n",
    "- how many tweets have been retweeted?\n",
    "- how many unique users?\n",
    "- how many tweets have not been retweeted?\n",
    "- most frequent hashtags\n",
    "\n",
    "**B. Wordcloud**\n",
    "- wordcloud of hashtags\n",
    "- generate ```bag_of_words``` from tweets\n",
    "- wordcloud of text\n",
    "\n",
    "**C. Retweet graph**\n",
    "\n",
    "- build network of retweet\n",
    "- find nodes with highest in-degree\n",
    "- compute other nodes' centralities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDZzPqzM0zWt"
   },
   "source": [
    "## A. Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM3PjRq80zWt"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(\"one.json\", \"rb\") as f: \n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "    data=pd.json_normalize(data) #to get as many nested information as possible into a single column in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asV7AEqI0zWt"
   },
   "outputs": [],
   "source": [
    "#liad data in a dataframe\n",
    "tweets_data = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvzyfNGsx_uD"
   },
   "outputs": [],
   "source": [
    "tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwODeXLF0zWt"
   },
   "outputs": [],
   "source": [
    "len(tweets_data.text.unique())  #check the amount of unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FgspcTk0zWt"
   },
   "outputs": [],
   "source": [
    "def print_retweets_unique_tweets_unique_users(tweets):\n",
    "    \n",
    "    retweets_ = tweets[\"public_metrics.retweet_count\"].apply(lambda x: 0 if x == 0 else 1) # to get the amount of tweets that have been retweeted. List of o/1 for retweet or not\n",
    "    tot_retweets = sum(retweets_) #number of tweets that have been retweeted\n",
    "    unique_tweets = len(retweets_) - tot_retweets# amiunt of tweets that have not been retweeted   \n",
    "    tot_users = tweets.author_id.unique()  #get number of unique users ids   \n",
    "    \n",
    "    print(\"tweets retweeted: {}\".format(tot_retweets))\n",
    "    print(\"tweets not retweeted: {}\".format(unique_tweets))\n",
    "    print(\"Unique Users: {}\".format(tot_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFi6HwSf0zWt"
   },
   "outputs": [],
   "source": [
    "print_retweets_unique_tweets_unique_users(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOkMUx7P0zWt"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#get text of the hashtags which are in the structure defined as \"tag\"\n",
    "def getTagsLists(lstTags):\n",
    "    listH=[]\n",
    "    for tg in lstTags:\n",
    "        listH.append(tg[\"tag\"].lower())\n",
    "    return listH\n",
    "\n",
    "def extract_hashtags():\n",
    "    hashtags_lists = tweets_data[\"entities.hashtags\"].dropna() #get rid of tweets that have no hashtags\n",
    "    hashtags = hashtags_lists.apply(lambda x: getTagsLists(x)) # get hashtags terms used (the text of a hashtag)\n",
    "    hashtags = list(itertools.chain(*hashtags)) #make a list of hashtags\n",
    "    hashtags_by_frequency = Counter(hashtags) #count the hashtags\n",
    "    hashtags_by_frequency = {k: hashtags_by_frequency[k] for k in hashtags_by_frequency if k != None} \n",
    "    \n",
    "    return hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV3ruRtX0zWu"
   },
   "outputs": [],
   "source": [
    "hashtags_by_frequency = extract_hashtags() \n",
    "hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTJLhgft0zWu"
   },
   "source": [
    "Let's plot the top-10 frequent hashtags now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQQMoy4M0zWu"
   },
   "outputs": [],
   "source": [
    "df_hashtags = pd.DataFrame(hashtags_by_frequency.items())\n",
    "df_hashtags.columns = [\"hashtag\", \"count\"]\n",
    "df_hashtags.set_index(\"hashtag\", inplace=True)\n",
    "df_hashtags.sort_values(\"count\", inplace=True, ascending=False) #order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAqlQrc80zWu"
   },
   "outputs": [],
   "source": [
    "df_hashtags.head(10).plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFVbCB5x0zWu"
   },
   "source": [
    "Let's try now to generate a wordcloud given the tweets we have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIwCOwet0zWu"
   },
   "source": [
    "# B. Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibCxNJQm0zWv"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S7zL33E0zWv"
   },
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "for text in tweets_data[\"text\"]:\n",
    "    \n",
    "    # remove \"RT\" string indicating a retweet\n",
    "    text = text.replace(\"RT \", \"\").strip()\n",
    "    \n",
    "    # lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all the punctuations\n",
    "    text = re.sub(r'[^\\w\\s]','',text).strip()\n",
    "    \n",
    "    # tokenize the text\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    lst_text = [x for x in lst_text if x not in STOPWORDS]\n",
    "    \n",
    "    # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "    for w in lst_text:\n",
    "        if w not in bag_of_words:\n",
    "            bag_of_words[w] = 0\n",
    "        bag_of_words[w] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQrbCznl0zWv"
   },
   "outputs": [],
   "source": [
    "def plot_wordcloud(title, dic_):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18,7))\n",
    "    wordcloud = WordCloud(background_color=\"white\",width=1600, height=800)\n",
    "    wordcloud = wordcloud.generate_from_frequencies(dic_)\n",
    "    ax.axis(\"off\")     \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szrfjC130zWv"
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud - All Tweets\", bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJPGXDOx0zWv"
   },
   "source": [
    "## C. Retweet graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTY80sjk0zWv"
   },
   "outputs": [],
   "source": [
    "#first we get all the collected tweets that are retweets, they are identified by an \"RT\" in the text\n",
    "df_retweets = tweets_data[tweets_data[\"text\"].apply(lambda x: x[:2]) == \"RT\"]\n",
    "df_retweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncAOA5shgNZI"
   },
   "outputs": [],
   "source": [
    "def getRetweetedUser(tweet):\n",
    "    return tweet.split(\":\")[0].split(\" \")[1].split(\"@\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es5S98PS0zWv"
   },
   "outputs": [],
   "source": [
    "#then we define that the nodes of our grap correspond to users, and a source node (retweeter) - destination node (original tweet author) pair is defined\n",
    "df_graph = pd.DataFrame(columns=[\"source\", \"destination\"])\n",
    "\n",
    "# add source-nodes (author_ids if retweeters)\n",
    "df_graph[\"source\"] = df_retweets['author_id']\n",
    "\n",
    "# add destination-nodes (author ids of original authors)\n",
    "df_graph[\"destination\"] = df_retweets[\"text\"].apply(lambda x: client.get_user(username= getReyweetedUser(x)).data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT4uYk4E0zWw"
   },
   "outputs": [],
   "source": [
    "df_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNtUb4wl0zWw"
   },
   "outputs": [],
   "source": [
    "df_graph.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEc53K5L0zWx"
   },
   "source": [
    "We now analyze the retweet graph, given by user-user interactions. The graph G=(V,E) is generate, where V is the set of users and a generic edge (u,v) is created if user u retweeted a post of the user v.\n",
    "\n",
    "We look at nodes properties, such in-degree, closeness centrality and pagerank.\n",
    "\n",
    "A fast recap of the measures we're goin to use:\n",
    "\n",
    "- **closeness centrality**: *it is a measure of centrality in a network, calculated as the reciprocal of the sum of the length of the shortest paths between the node and all other nodes in the graph. Thus, the more central a node is, the closer it is to all other nodes* ([link](https://en.wikipedia.org/wiki/Closeness_centrality)).\n",
    "\n",
    "- **pagerank**: It defines a probability distribution over all the nodes in the graph. A score/probability assigned to each node indicates the importance of the single node, taking into account both local and global structure of the graph ([link](https://en.wikipedia.org/wiki/PageRank)).\n",
    "\n",
    "\n",
    "**References to the networkx library:**\n",
    "\n",
    "Graphs: https://networkx.org/documentation/stable/reference/classes/digraph.html\n",
    "\n",
    "Page rank: https://networkx.org/documentation/stable/reference/algorithms/link_analysis.html#module-networkx.algorithms.link_analysis.pagerank_alg\n",
    "\n",
    "Closeness: https://networkx.org/documentation/stable/reference/algorithms/centrality.html#closeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3rFUKeq0zWx"
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFAuoEnM0zWx"
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(df_graph.values)\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jYNTtaT0zWy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topk_indegree = sorted(G.in_degree(), key=lambda x: x[1], reverse=True)[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgpZK3JV0zWy"
   },
   "outputs": [],
   "source": [
    "topk_indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzQvBg1v0zWy"
   },
   "outputs": [],
   "source": [
    "topk_pagerank =  sorted(nx.pagerank(G).items(), key=lambda x: x[1], reverse=True)[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPIRpEXJ0zWy"
   },
   "outputs": [],
   "source": [
    "topk_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVhE5xl20zWy"
   },
   "outputs": [],
   "source": [
    "topk_closeness = sorted(nx.closeness_centrality(G).items(), key=lambda x: x[1], reverse=True)[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVJ_tpvi0zWz"
   },
   "outputs": [],
   "source": [
    "topk_closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLNX9NSH0zWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgEObNHLl_8P"
   },
   "source": [
    "2nd Approach - Collecting Tweets using Responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra6l6tfzmIE6"
   },
   "outputs": [],
   "source": [
    "user_name = 'Netflix'\n",
    "user_id=client.get_user(username=user_name).data.id #first we get the user id using the username as argument\n",
    "tweetsResponses = []\n",
    "next_token = None\n",
    "\n",
    "for i in range(33):\n",
    "    if next_token == None:\n",
    "    tweets = client.get_users_tweets(id=user_id, tweet_fields=['id','text','created_at','public_metrics'],\n",
    "                                max_results=100)\n",
    "    else:\n",
    "    tweets = client.get_users_tweets(id=user_id, tweet_fields=['id','text','created_at','public_metrics'],\n",
    "                                max_results=100,pagination_token=next_token)\n",
    "    tweetsResponses.append(tweets)      \n",
    "    metadata = tweets.meta\n",
    "    next_token = metadata.get(\"next_token\")\n",
    "    print(next_token)\n",
    "    if next_token == None:\n",
    "        print(\"Limit Exceeded\")\n",
    "        break\n",
    "\n",
    "#A collections.namedtuple, with data, includes, errors, and meta fields, corresponding with the fields in responses from Twitterâ€™s API.\n",
    "print(len(tweetsResponses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LQAKgXZmKz-"
   },
   "outputs": [],
   "source": [
    "all_tweets = {\"id\":[],\"text\":[],\"created_at\":[],\"public_metrics\":[],\"tweepy\":[]}\n",
    "for tResponses in tweetsResponses: #tweepy.client.Response\n",
    "  #print(type(tResponses),tResponses.data)\n",
    "  for tweet in tResponses.data: #tweepy.tweet.Tweet (id,text,created_at,public_metrics)\n",
    "    #print(type(tweet))\n",
    "    all_tweets[\"id\"].append(tweet.id)\n",
    "    all_tweets[\"text\"].append(tweet.text)\n",
    "    all_tweets[\"created_at\"].append(tweet.created_at)\n",
    "    all_tweets[\"public_metrics\"].append(tweet.public_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnYlmD4bmMn7"
   },
   "outputs": [],
   "source": [
    "len(all_tweets[\"id\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
