{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alfa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1575911964555247616, 'text': '#HurricaneIan expected to make less severe landfall impact in South Carolina within next 1-2 hours. https://t.co/IS8aEbO90o', 'username': 'AviationEnthusiastsClub', 'date': 'Fri Sep 30 18:14:26 +0000 2022', 'hashtag': [{'text': 'HurricaneIan', 'indices': [0, 13]}], 'like': 0, 'rt': 0, 'URL': 1}\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'data/tw_hurricane_data.json'\n",
    "tweets_title = 'data/tweet_document_ids_map.csv'\n",
    "\n",
    "tweets_id_title = {}\n",
    "\n",
    "with open(tweets_title) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "\n",
    "for l in lines:\n",
    "    l = l.strip().split(\"\\t\")\n",
    "    tweets_id_title[int(l[1])] =  l[0]\n",
    "\n",
    "\n",
    "tweets = []\n",
    "lines = []\n",
    "\n",
    "for line in open(docs_path, 'r'):\n",
    "    lines.append(line)\n",
    "    #media = json.loads(line).get('entities').get('media')\n",
    "    tweets.append({\n",
    "        'id' : int(json.loads(line).get('id')),\n",
    "        'text': json.loads(line).get('full_text'),\n",
    "        'username' : json.loads(line).get('user').get('name'),\n",
    "        'date' : json.loads(line).get('created_at'),\n",
    "        'hashtag' : json.loads(line).get('entities').get('hashtags'), # Si a√±adimos .get('text') obtenemos solo info del hashtag\n",
    "        'like' : json.loads(line).get('favorite_count'),\n",
    "        'rt' : json.loads(line).get('retweet_count'),\n",
    "        'URL' : len(json.loads(line).get('entities').get('urls'))\n",
    "    }) #Here we get only the text from the tweets in json document\n",
    "print(tweets[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Tweets: {}\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will clean our text from data that is not important so that has no weight \n",
    "def clean_text(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "   \n",
    "    tweet = tweet.lower() # Transform in lowercase\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Here we remove punctuation marks\n",
    "    tweet = tweet.split() # Tokenize the text to get a list of terms\n",
    "    tweet = [word for word in tweet if word not in stop_words]  # eliminate the stopwords\n",
    "    #tweet = [stemmer.stem(tweet) for tweet in tweets] # Perform stemming \n",
    "    ## END CODE    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'expanded', 'community', 'foundations', 'across', 'florida', 'set', 'relief', 'funds', 'help', 'impacted', 'hurricaneian', 'take', 'action', 'share', 'cfsarasota', 'manateecf', 'colliercffl', 'givecf', 'miamifoundation', 'httpstco0uui16fita']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(tweets[1000].get('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)    \n",
    "\n",
    "    for i in range(len(tweets)):\n",
    "        terms = clean_text(tweets[i].get(\"text\"))\n",
    "        tweet_id = tweets[i].get('id')\n",
    "\n",
    "        tweet_title = tweets_id_title[tweet_id]\n",
    "\n",
    "        if tweet_title != None:\n",
    "            tweet_title = tweet_title.get(\"title\")\n",
    "\n",
    "        current_tweet_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "\n",
    "                ## START CODE\n",
    "                current_tweet_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_tweet_index[term] = [tweet_title, array('I', [position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term, posting_page in current_tweet_index.items():\n",
    "            index[term].append(posting_page)\n",
    "\n",
    "        ## END CODE                    \n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_20589/2791291513.py\", line 1, in <module>\n",
      "    index = create_index(tweets)\n",
      "  File \"/tmp/ipykernel_20589/3619136245.py\", line 18, in create_index\n",
      "    tweet_title = next((item for item in tweets_id_title if item[\"id\"] == tweet_id), None)\n",
      "  File \"/tmp/ipykernel_20589/3619136245.py\", line 18, in <genexpr>\n",
      "    tweet_title = next((item for item in tweets_id_title if item[\"id\"] == tweet_id), None)\n",
      "TypeError: 'int' object is not subscriptable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 839, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, \"bg:ansiyellow\")\n",
      "  File \"/home/alfa/.local/lib/python3.8/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "index = create_index(tweets)\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['house']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['house'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = clean_text(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"\n",
    "            term_docs = [posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 0 for the searched query:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, tweets_id_title[d_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
