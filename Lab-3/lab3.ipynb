{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "# from config import *\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will clean our text from data that is not important so that has no weight \n",
    "def clean_text(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "   \n",
    "    tweet = tweet.lower() # Transform in lowercase\n",
    "\n",
    "    tweet = re.sub(r'@[a-zA-Z]+', '', tweet) # Here we remove the mentions in the tweet ex: @canodep\n",
    "    tweet = re.sub(r\"\\B#([a-z0-9]{2,})(?![~!@#$%^&*()=+_`\\-\\|\\/'\\[\\]\\{\\}]|[?.,]*\\w)\", '', tweet) # Here we remove the hashtags, because we will treat it later\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Here we remove punctuation marks\n",
    "    tweet = re.sub(r'http\\S+', '',tweet) # Remove http and https\n",
    "    tweet = tweet.split() # Tokenize the text to get a list of terms\n",
    "\n",
    "    tweet = [word for word in tweet if word not in stop_words]  # eliminate the stopwords\n",
    "    tweet = [stemmer.stem(word) for word in tweet] # Perform stemming \n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = 'data/tw_hurricane_data.json'\n",
    "tweets_title = 'data/tweet_document_ids_map.csv'\n",
    "\n",
    "tweets_id_title = {}\n",
    "\n",
    "with open(tweets_title) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "\n",
    "for l in lines:\n",
    "    l = l.strip().split(\"\\t\")\n",
    "    tweets_id_title[int(l[1])] =  l[0]\n",
    "\n",
    "\n",
    "tweets = []\n",
    "lines = []\n",
    "\n",
    "for line in open(docs_path, 'r'):\n",
    "    lines.append(line)\n",
    "    #media = json.loads(line).get('entities').get('media')\n",
    "    tweets.append({\n",
    "        'id' : int(json.loads(line).get('id')),\n",
    "        'title' : tweets_id_title[int(json.loads(line).get('id'))],\n",
    "        'text': json.loads(line).get('full_text'),\n",
    "        'username' : json.loads(line).get('user').get('screen_name'),\n",
    "        'date' : json.loads(line).get('created_at'),\n",
    "        'hashtag' : list(map(lambda hashtag:  hashtag.get('text'),  json.loads(line).get('entities').get('hashtags'))),\n",
    "        'like' : json.loads(line).get('favorite_count'),\n",
    "        'rt' : json.loads(line).get('retweet_count'),\n",
    "        'URL' : 'https://twitter.com/' + json.loads(line).get('user').get('screen_name') + \"/status/\" + str(json.loads(line).get('id'))\n",
    "    }) \n",
    "\n",
    "tweets_texts = [tweet['text'] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFequency(term, document):\n",
    "    return document.count(term) / len(document)\n",
    "\n",
    "def inverseDocumentFrequency(term, documents):\n",
    "    n = 0\n",
    "    for doc in documents:\n",
    "        if term.lower() in doc:\n",
    "            n += 1\n",
    "    return 1.0 + np.log(float(len(documents)) / n) if (n > 0) else 1.0\n",
    "\n",
    "\n",
    "def tfiidf(term, document, documents):\n",
    "    tf = termFequency(term, document)\n",
    "    idf = inverseDocumentFrequency(term, documents)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "tfiidf('neighborhood', clean_text(tweets[2]['text']), tweets_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Help and during the hurricane disaster\"\n",
    "def generateVectors(query, documents):\n",
    "    query = clean_text(query)\n",
    "    tf_idf_matrix = np.zeros((len(query), len(documents)))\n",
    "    for i, term in enumerate(query):\n",
    "        idf = inverseDocumentFrequency(term, documents)\n",
    "        for j, document in enumerate(documents):\n",
    "            tf_idf_matrix[i][j] = idf * termFequency(term, document)\n",
    "    return tf_idf_matrix\n",
    "\n",
    "tf_idf_matrix = generateVectors(query, tweets_texts)\n",
    "\n",
    "def word_count(query):\n",
    "    query = clean_text(query)\n",
    "    count = dict()\n",
    "    for word in query:\n",
    "        if word in count:\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    return count\n",
    "\n",
    "def queryVector(query, documents):\n",
    "    count = word_count(query)\n",
    "    vector = np.zeros((len(count),1))\n",
    "    for i, word in enumerate(clean_text(query)):\n",
    "        vector[i] = float(count[word])/len(count) * inverseDocumentFrequency(word, documents)\n",
    "    return vector\n",
    "\n",
    "query_vector = queryVector(query, tweets_texts)\n",
    "\n",
    "\n",
    "def cosineSimilarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet id: 1575886758075678720 | tweet title: doc_1921 | relevance: [[1.]]\n",
      "Tweet id: 1575874361675976705 | tweet title: doc_2610 | relevance: [[1.]]\n",
      "Tweet id: 1575870774027911168 | tweet title: doc_2868 | relevance: [[1.]]\n",
      "Tweet id: 1575880023617437696 | tweet title: doc_2224 | relevance: [[0.95129964]]\n",
      "Tweet id: 1575911845927567375 | tweet title: doc_509 | relevance: [[0.95101843]]\n",
      "Tweet id: 1575905585320497152 | tweet title: doc_1048 | relevance: [[0.95101843]]\n",
      "Tweet id: 1575917149356691457 | tweet title: doc_68 | relevance: [[0.90621033]]\n",
      "Tweet id: 1575917131564097536 | tweet title: doc_73 | relevance: [[0.90621033]]\n",
      "Tweet id: 1575908420355379201 | tweet title: doc_834 | relevance: [[0.90621033]]\n",
      "Tweet id: 1575905473261293570 | tweet title: doc_1057 | relevance: [[0.90621033]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124505/3053642508.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def compute_relevance(query, tweets):\n",
    "    # Calculate relevance with cosine similarity\n",
    "    documents = [tweet['text'] for tweet in tweets]\n",
    "    query_vector = queryVector(query, documents)\n",
    "    tf_idf_matrix = generateVectors(query, documents)\n",
    "    relevance = []\n",
    "    \n",
    "    for i, document in enumerate(documents):\n",
    "        relevance_i = (cosineSimilarity( tf_idf_matrix[:, i].reshape(1, -1), query_vector))\n",
    "        relevance.append(relevance_i if math.isnan(relevance_i) == False else 0.0)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tweet['relevance'] = relevance[i]\n",
    "        \n",
    "    return sorted(tweets, key=lambda tweet: tweet['relevance'], reverse=True)\n",
    "    \n",
    "tweets_ranked = compute_relevance(query, tweets)[:10]\n",
    "\n",
    "for tweet in tweets_ranked:\n",
    "    print(\"Tweet id: {} | tweet title: {} | relevance: {}\".format(tweet[\"id\"], tweet[\"title\"], tweet[\"relevance\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Rank\n",
    "We will sort the documents that contain the query by like and RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet id: 1575908406355197952 | tweet title: doc_838 | relevance: 1645\n",
      "Tweet id: 1575864357845495809 | tweet title: doc_3397 | relevance: 1222\n",
      "Tweet id: 1575906862829953025 | tweet title: doc_970 | relevance: 1142\n",
      "Tweet id: 1575863704222019585 | tweet title: doc_3458 | relevance: 563\n",
      "Tweet id: 1575894492443336714 | tweet title: doc_1586 | relevance: 481\n",
      "Tweet id: 1575859547012534273 | tweet title: doc_3743 | relevance: 400\n",
      "Tweet id: 1575861505509462021 | tweet title: doc_3623 | relevance: 391\n",
      "Tweet id: 1575875845125771270 | tweet title: doc_2493 | relevance: 313\n",
      "Tweet id: 1575910903425695745 | tweet title: doc_581 | relevance: 270\n",
      "Tweet id: 1575857697265635328 | tweet title: doc_3892 | relevance: 255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def customRank(query, tweets):\n",
    "    # CAlculate the relevance by the times query appears in the tweet\n",
    "    # and the number of likes and retweets\n",
    "\n",
    "    for word in clean_text(query):\n",
    "        for tweet in tweets:\n",
    "            tweet['relevance'] = 0\n",
    "            if word in clean_text(tweet['text']):\n",
    "                tweet['relevance'] += 1\n",
    "            tweet['relevance'] += tweet['like'] + tweet['rt']\n",
    "            \n",
    "    return sorted(tweets, key=lambda tweet: tweet['relevance'], reverse=True)\n",
    "        \n",
    "    \n",
    "tweets_ranked = customRank(\"work\", tweets)[:10]\n",
    "\n",
    "for tweet in tweets_ranked:\n",
    "    print(\"Tweet id: {} | tweet title: {} | relevance: {}\".format(tweet[\"id\"], tweet[\"title\"], tweet[\"relevance\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet id: 1575878683675578370 | tweet title: doc_2293 | relevance: 0.12596235467069963\n",
      "Tweet id: 1575870693698568193 | tweet title: doc_2880 | relevance: 0.11963651580071205\n",
      "Tweet id: 1575909937137565723 | tweet title: doc_673 | relevance: 0.11568219971711384\n",
      "Tweet id: 1575858054683205632 | tweet title: doc_3861 | relevance: 0.11192204434229668\n",
      "Tweet id: 1575884021573132288 | tweet title: doc_2033 | relevance: 0.10661860886047461\n",
      "Tweet id: 1575915585505878016 | tweet title: doc_207 | relevance: 0.09419688566175363\n",
      "Tweet id: 1575869948886335488 | tweet title: doc_2956 | relevance: 0.09009614357279695\n",
      "Tweet id: 1575884212347211776 | tweet title: doc_2023 | relevance: 0.08553299920433532\n",
      "Tweet id: 1575868031473168385 | tweet title: doc_3116 | relevance: 0.08526481354421626\n",
      "Tweet id: 1575862322324611075 | tweet title: doc_3573 | relevance: 0.08151415751233337\n"
     ]
    }
   ],
   "source": [
    "def bm25(query, tweets, k1=1.5, b=0.75):\n",
    "    # Calculate relevance with BM25 algorithm using k1=1.5 and b=0.75\n",
    "    documents = [tweet['text'] for tweet in tweets]\n",
    "    query = clean_text(query)\n",
    "    tf_idf_matrix = np.zeros((len(query), len(documents)))\n",
    "    for i, term in enumerate(query):\n",
    "        idf = inverseDocumentFrequency(term, documents)\n",
    "        for j, document in enumerate(documents):\n",
    "            tf_idf_matrix[i][j] = idf * termFequency(term, document)\n",
    "    relevance = []\n",
    "    for i, document in enumerate(documents):\n",
    "        relevance.append(np.sum(tf_idf_matrix[:, i] * (k1 + 1) / (tf_idf_matrix[:, i] + k1 * (1 - b + b * len(document) / np.mean([len(doc) for doc in documents])))))\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tweet['relevance'] = relevance[i]\n",
    "    \n",
    "    return sorted(tweets, key=lambda tweet: tweet['relevance'], reverse=True)\n",
    "        \n",
    "    \n",
    "tweets_ranked = bm25(\"work\", tweets)[:10]\n",
    "\n",
    "for tweet in tweets_ranked:\n",
    "    print(\"Tweet id: {} | tweet title: {} | relevance: {}\".format(tweet[\"id\"], tweet[\"title\"], tweet[\"relevance\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
